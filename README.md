# COMM4190 Spring 2024 - Research Project: *Social Penetration Theory  and LLMs' Limitations in Mental Health Support*

Social Penetration Theory (SPT) was developed by psychologists Irwin Altman and Dalmas Taylor in 1973. It offers a framework to understand how relational closeness evolves through communication. The theory compares someone's personality to an onion with multiple layers, suggesting that deeper interpersonal connections are forged as these layers are peeled back through self-disclosure. Central to SPT is the concept of self-disclosure, defined as the voluntary sharing of personal information with others. This process is characterized by both the breadth (the range of topics discussed) and the depth (the intimacy level of the topics) of information shared. As relationships develop, interactions move from superficial layers involving basic information and casual conversation to deeper, more personal layers. This progression not only facilitates closer bonds but also increases vulnerability as more personal information is disclosed. The theory provides a valuable lens for examining how trust and intimacy evolve in relationships, making it particularly relevant for analyzing interactions with AI systems like LLMs, where simulated self-disclosure might play a role in user engagement and trust.

## Therapy Interactions and Social Penetration Theory

In therapeutic settings, open and honest communication is very important for building trust and fostering the judgment-free environment that it is supposed to be. However, clients often find it challenging to fully disclose sensitive information, even when confidentiality is guaranteed and empathy is the top priority. Social Penetration Theory can provide a valuable framework for understanding how relationships deepen through layers of self-disclosure, especially in this therapeutic context. The theory likens personalities to onions, with multiple layers representing progressively intimate topics that are revealed over time as trust develops between individuals. With the rise of more and more advanced LLMs, there's increasing interest in exploring their potential role in facilitating therapeutic conversations. Although these AI systems obviously cannot fully replace human therapists, their ability to simulate empathy, provide judgment-free support, and respond dynamically to user input could help them be used as potential tools for supplementing traditional therapy. By offering 24/7 availability and immediate responses, LLMs could help reduce barriers to mental health support and possibly make it more effective.

However, challenges arise when using LLMs in therapeutic contexts, particularly concerning their ability to simulate the delicate process of self-disclosure and relational development. Some individuals may find it difficult to fully disclose their thoughts and emotions, even with an AI in a seemingly non-judgmental space. This project aims to explore therapy interactions through the lens of Social Penetration Theory, examining whether and how LLMs can encourage trust and openness over time. By understanding the layers of disclosure and trust-building strategies in these interactions, we can better gauge the implications and ethical considerations of using AI systems in mental health support.

This report will review relevant literature, design and analyze simulated therapy interactions with LLMs, and explore how effectively these systems can replicate the depth of relational closeness. I will also highlight the potential opportunities and challenges posed by LLMs in supporting mental health care.

## Background

**Social Penetration Theory in Therapy**
Social Penetration Theory explains how interpersonal relationships develop through a kind of strategic exchange of personal information over time. In therapy, this theory can be particularly insightful because the therapeutic process fundamentally relies on deepening intimacy through progressive self-disclosure. Therapists and clients go from superficial to more intimate layers of personal experiences and emotions, which is obviously important for effective therapy.

**Challenges of Self-Disclosure in Therapy**
Despite the safe space that therapy aims to provide, many clients still struggle with self-disclosure. There can be many reasons, ranging from fear of judgment and vulnerability to more overarching cultural stigmas associated with mental health. Properly understanding these barriers is essential for assessing how well LLMs might facilitate or hinder these conversations.

**Trust and Empathy**
Trust and empathy are cornerstones of therapeutic relationships. Trust is built as therapists consistently respond with understanding and support, validating the client's feelings without judgment. On the other hand, empathy involves the therapist's ability to perceive and even share the feelings of the client.

*This background sets the stage for exploring how LLMs could potentially replicate or facilitate the complex dynamics of therapy, specifically under the framework of Social Penetration Theory. The following section will focus on the existing **literature** related to Social Penetration Theory and its application in therapeutic interactions, as well as exploring studies on how LLMs these systems handle trust-building, empathy, and self-disclosure in therapy-like settings.*

## Literature Review

As discussed in the previous section, SPT describes how relational closeness develops through progressive self-disclosure, making it a valuable framework for understanding trust and intimacy in therapeutic interactions.

**Artificial Intelligence and Empathy in Therapeutic Settings**

Recent advancements in artificial intelligence have sparked significant interest in how machine-driven empathy could enhance therapeutic communication. With the rise of chatbots and conversational agents like ChatGPT, researchers have been exploring the extent to which LLMs can simulate empathetic responses and provide mental health support. A 2021 study by Miner et al. showed that users perceive conversational agents as more empathetic when responses are personalized and linguistically similar to their own language. For instance, when a participant expressed feeling overwhelmed at work, the chatbot replied with a personalized response like, "I understand that work can be stressful at times, especially with tight deadlines. Taking a short break or jotting down your tasks for the day might help you feel more organized." Such responses, tailored to the user's unique situation and needs, demonstrated that personalized language could significantly improve the perception of empathy in AI-driven interactions. Participants found the chatbot's responses supportive, relevant, and emotionally attuned. This suggests that response customization is crucial in promoting user satisfaction and fostering a sense of connection.

However, empathy delivered by AI clearly has limitations, namely being more context-blind and lacking human, nuanced empathy. A 2022 study by de Greef et al. found that while LLMs can mimic empathetic language, they struggle to offer nuanced emotional support in situations requiring contextual awareness. In this study, the researchers created emotionally charged scenarios, such as a person expressing severe anxiety or deep grief. In one case, when a user conveyed distress over a significant loss, the LLM responded with a generic "I'm sorry to hear that," followed by advice to "stay positive and things will get better." Such responses, while well-intentioned, were perceived as inadequate and lacking contextual understanding due to their failure to acknowledge the user's specific circumstances or emotions. These limitations often lead to inappropriate or generic responses during emotionally charged interactions. LLMs often lack the deep understanding of human emotions that therapists develop through training and experience, which can affect their ability to respond to complex therapeutic needs, as displayed through de Greef's study.

Overall, while LLMs show promise in simulating empathetic responses and providing accessible mental health support, the lack of emotional intuition and contextual awareness limits their ability to fully replicate human empathy at this current moment. Future research and development is needed to improve the emotional intelligence of these systems to ensure they can offer more meaningful and tailored support in therapeutic settings.

**Trust and Self-Disclosure in Human-AI Interactions**

As discussed, trust is the most central component of effective communication in therapy, influencing the depth and breadth of client self-disclosure. A systematic review by Lucas et al. (2020) outlined that trust in AI systems is influenced by response accuracy, perceived empathy, and transparency in how data is used. Self-disclosure can be inhibited by concerns over privacy and a lack of authentic, emotionally resonant responses from the AI system. 

Despite these challenges, other research, like a 2019 study by Fritz et al., indicates that users may feel more inclined to share personal information with machines because of the perception that they offer a non-judgmental, anonymous space. Still, privacy concerns may still limit disclosure if users worry about how their data will be stored or used. This dichotomy suggests that AI systems can simultaneously encourage and inhibit self-disclosure depending on design, transparency, and perceived reliability. This paradox indicates that AI systems have the potential to both encourage and inhibit self-disclosure based on the specific design of their interactions.

Users are more likely to trust an AI that responds appropriately to their specific needs and adapts its conversational style based on the user's cues. Also, the lack of a real human can lead to a perception of much less judgment. However, the potential for emotionally insensitive or generic responses remains a challenge that could inhibit deeper levels of disclosure. 

Ultimately, the trust and self-disclosure dynamic in human-AI interactions relies heavily on the balance between providing non-judgmental spaces and maintaining transparent data practices. Further research is required to understand how best to design these systems to foster trust while managing ethical concerns and data privacy.

**Conclusion of Literature Review**

Existing literature provides a foundation for understanding the potential and limitations of using LLMs in therapeutic interactions. While Social Penetration Theory offers insights into relational development, applying it to AI interactions raises questions about the depth of trust and empathy that machines can realistically replicate. Further exploration is needed to determine how effectively LLMs can foster trust and encourage disclosure, particularly given the ethical and emotional complexities of therapeutic conversations.

## Implications of Using LLMs in Therapy Interactions: Through the Lens of SPT

The integration of LLMs in therapy offers promising opportunities but also significant challenges, particularly when examined through the lens of SPT. This theory emphasizes the gradual deepening of relational intimacy through progressive self-disclosure, and it raises questions about how LLMs might facilitate or hinder such relational development.

1. Self-Disclosure and Relational Closeness:
LLMs can mimic human-like conversations, which may encourage users to gradually disclose personal information in layers as trust builds. However, the absence of true human empathy could result in less nuanced responses that lack the emotional depth necessary to foster genuine relational closeness. Despite their simulated empathetic responses, LLMs might not adequately respond to clients’ specific emotions, leading to superficial relationships that lack the richness of real therapeutic bonds. This superficiality risks reducing SPT’s progression through layers of intimacy to a transactional exchange that likely will miss deeper emotional connections.

2. Trust Dynamics:
In traditional therapy, trust is earned through consistent empathy, reliability, and a judgment-free approach. With LLMs, the trust dynamic is altered as users rely on the AI's perceived competence, accuracy, and personalization. However, building trust through SPT's gradual layers of interaction can be challenging when the LLMs lack a genuine understanding of human emotions. Inconsistencies in response quality or perceived insensitivity could quickly erode trust, leading to reduced self-disclosure. Thus, developing AI systems that can adjust their conversational depth and tone based on user cues becomes essential.

3. Ethical Impact on Self-Disclosure:
Ethical considerations are critical, as they directly affect the willingness of users to disclose personal information. SPT suggests that vulnerability increases as deeper layers of self-disclosure are reached. However, without robust privacy measures, clients may withhold information due to concerns about data misuse. This reluctance could prevent meaningful therapeutic progress, limiting the ability of LLMs to create spaces where clients feel secure enough to share openly. Hence, transparency in how user data is handled is paramount to fostering deeper relational intimacy.

4. AI and the Potential to Normalize Self-Disclosure:
One positive implication is that LLMs might help normalize discussions around mental health by providing immediate, stigma-free support. For some clients, especially those uncomfortable with traditional therapy, LLMs can provide a non-judgmental space that encourages self-disclosure. This aligns well with SPT’s emphasis on reciprocity, as users can engage in conversations without fear of stigma, possibly reducing barriers to traditional therapeutic support.

*Overall, applying Social Penetration Theory to therapy interactions with LLMs highlights both opportunities and challenges. While LLMs can promote gradual self-disclosure and potentially encourage more people to seek help, their lack of genuine empathy may hinder the formation of deeper therapeutic relationships. The ethical implications, particularly around privacy and data usage, must also be addressed to maintain trust and relational closeness. Understanding these dynamics can guide the responsible integration of LLMs in therapy, ensuring they supplement rather than replace human therapists and uphold the fundamental principles of empathetic care.*

## Conclusion

Therapeutic interactions with LLMs present both exciting opportunities and significant challenges, particularly through the lens of Social Penetration Theory. While these systems can help to foster gradual self-disclosure and provide immediate, stigma-free support that human therapists may lack, they lack the nuanced empathy and contextual understanding needed to fully replicate the deep relationships built through that traditional therapy. Trust, relational closeness, and ethical considerations all play very important roles in ensuring that these AI systems are actually beneficial rather than detrimental.

To improve LLMs and address their limitations, several specific, actionable steps can be taken:

**Enhanced Empathy Simulation:**
    - *Contextual Awareness:* Train LLMs to recognize and respond to emotional tones. Diverse datasets containing varied and nuanced emotional expressions should be curated and labeled, including data from forums, mental health transcripts, and social media. Fine-tune the LLM using emotion detection models and multi-task learning to reinforce emotional recognition, and incorporate human feedback loops to refine responses, ensuring empathetic, culturally aware interactions.
    - *Emotion-Specific Prompts:* Develop prompts or conversation structures that allow the LLMs to simulate specific empathetic responses, such as validation, encouragement, or reflective listening, tailored to the emotional state of the user.

**Adaptive Conversation Strategies:**
    - *Personalization Engine:* Building off of current memory features that some LLMs like GPT-4 offer, a personalization engine should be implemented that can refine user profiles by analyzing previous interactions, learning users' communication styles, preferences, and emotional needs. It should continuously update its understanding to offer tone and content tailored to each user, remembering specific concerns and adjusting the conversation approach to align with past emotional states. By integrating user feedback and maintaining session continuity (acting more and more like a human would), the LLM can deliver more empathetic/relevant responses through many sessions.
    - *Dialogue Depth Analysis:* A dialogue depth analysis system would categorize user responses based on their level of intimacy, from superficial topics to deeply personal concerns, aligning with Social Penetration Theory's layers of disclosure. The LLM would then tailor its responses to match the user's current disclosure level while gradually encouraging deeper conversations as trust builds. This adaptive approach ensures sensitive topics are handled with care, fostering meaningful therapeutic relationships by respecting boundaries and responding empathetically.

**Ethical Design and Transparency:**
    - *Clear Data Policies:* Clearly communicate data usage and privacy policies to users, offering them control over what information is stored or shared, which will build trust and encourage deeper disclosure.
    - *Consent-Based Feedback Collection:* Collect feedback on user interactions through voluntary, consent-based surveys to identify areas where the LLM fails to meet ethical or empathetic standards and improve accordingly.
    - *This should be something applied to all areas of AI application*

**Crisis Detection and Support:**
    - *Crisis Recognition Algorithms:* Crisis recognition algorithms help identify users in distress by analyzing conversation data and applying machine learning models. This process starts with recording and matching (as my MKTG AI Professor, Prof Puntoni, describes it) datasets containing crisis-related examples, like hotline transcripts and mental health forums, to train the algorithm on signals of suicidal ideation, panic attacks, or self-harm. By then extracting linguistic patterns such as negative sentiment, sudden shifts in tone, etc., they can be combined with classification models to help distinguish between normal and critical conversations. As previously stated, incorporating contextual awareness through conversation history and cultural sensitivity improves the algorithm's accuracy. Human therapists can validate flagged conversations to refine the models, while escalation protocols ensure urgent cases are referred to emergency services or local resources (another Puntoni reference: the Funnel Approach). This approach can help to make sure that users receive effective support when a crisis is detected.
    - *Human Escalation Protocols:* Develop escalation protocols where interactions flagged as critical can be swiftly directed to a trained human therapist for intervention (The Funnel Approach).

**Therapist Collaboration:**
    - *Support Tool Integration:* Design LLMs as supportive tools that can be integrated into existing therapy practices, where human therapists review AI recommendations and provide personalized follow-up.
    - *Hybrid Models:* Explore hybrid models where LLMs conduct initial assessments or provide continuous check-ins while human therapists focus on more complex therapeutic relationships.

*Implementing these improvements will require a concerted effort across the tech and mental health sectors. However, with the right strategies, LLMs can supplement traditional therapy by offering immediate, empathetic support that remains firmly rooted in ethical and personalized care. By addressing their limitations thoughtfully, LLMs can positively impact therapeutic interactions while respecting the sensitive nature of mental health care.*

In therapeutic interactions, applying Social Penetration Theory reveals that the progression of intimacy and trust relies heavily on gradual layers of self-disclosure. LLMs, designed to simulate empathetic responses and adapt to user input, can foster these layers by offering personalized, stigma-free conversations that encourage deeper sharing over time. However, the lack of genuine empathy and contextual awareness limits their ability to truly replicate human relational depth, often resulting in superficial exchanges rather than meaningful bonds. Crisis recognition algorithms, personalization engines, and dialogue depth analysis offer promising solutions to these challenges, helping tailor responses and detect critical situations, but maintaining trust requires a stronger grasp of the emotional nuances essential for therapeutic engagement.

In conclusion, the implications of SPT for AI therapy conversations are significant. While LLMs can simulate the process of relational development through thoughtful adaptation and personalized responses, their current limitations in understanding the full emotional depth of human interactions mean they often lack the nuanced empathy required for authentic therapeutic connections. Improving these systems requires the refinement of crisis recognition, emotional sensitivity, and continuity across sessions to align more closely with SPT’s principles of gradual, reciprocal intimacy. Incorporating dynamic user profiling, feedback integration, and escalation protocols will allow LLMs to complement traditional therapy while ensuring that the ethical, relational, and practical considerations inherent to SPT remain a central focus.